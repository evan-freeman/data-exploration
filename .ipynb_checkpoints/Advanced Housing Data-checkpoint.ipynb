{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Advanced Housing Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Table of contents\n",
    "2. Finish doing one-hot plus my ordinal encoding\n",
    "3. Figure out how to use unprocessed categorical features with LightGBM (it says it can do it, but I haven't been able to figure out how. Maybe I have to convert from a pandas dataframe to whatever datastructure LGB prefers?)\n",
    "4. Dimensionality reductions / PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Hello. My name is Evan Freeman. Let's analyze a housing dataset and create a model to predict housing prices of new houses that are put on the market.\n",
    "\n",
    "Here we'll be considering the Ames Housing dataset, which is like the famous Boston Housing dataset, but better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "1. Split Validation Data (Test data is already seperate, Kaggle will test my model on that data)\n",
    "  1. 80 / 20 Split\n",
    "  2. k-fold cross validation\n",
    "    1. I'll have to rerun the whole pipeline for each fold\n",
    "    2. I've read that 5 or 10 folds are good. Not sure why exactly those numbers.\n",
    "2. Visually investigate the data\n",
    "3.  Feature Generations \n",
    "  1. Categorical Parings\n",
    "  2. Numerical Transformations \n",
    "4.  Feature Selection\n",
    "  1. Principal Component Analysis???\n",
    "  2. Univariate\n",
    "  3. Lasso (L1)\n",
    "  4. Ridge (L2)\n",
    "5.  Try different models\n",
    "  1.  Simple Regression\n",
    "  2.  Random Forest\n",
    "  3.  XGBoost\n",
    "  4.  LightGBM\n",
    "  5.  Neural Net (lol wut)\n",
    "6.  Hyperparameter Tuning (my fav)\n",
    "7.  Compare them!\n",
    "\n",
    "*This is a lie currently, will update someday..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 56\n",
    "\n",
    "\n",
    "filepath = 'D:/code/Data/house-prices-advanced-regression-techniques/'\n",
    "sub_filepath = 'D:/code/Data/advanced_housing_submissions/'\n",
    "\n",
    "housing = pd.read_csv(f'{filepath}train.csv')\n",
    "test = pd.read_csv(f'{filepath}test.csv')\n",
    "\n",
    "# Also, we're going to need the test ids for submission, so let's grab those\n",
    "\n",
    "test_id = test['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The test data has already been split for us by Kaggle, but we MUST split off validation data (or do k-fold) as well!!! Otherwise we will overfit.  \n",
    "\n",
    "Let's split off 20% for validation.  \n",
    "\n",
    "Later, we'll try with k-fold validation, which should produce better results.  \n",
    "\n",
    "It is worth pointing out that, once we've trained and validated our model, chosing the best type of model, the best features, the best hyperparameters, we should go back and train it from scratch on the entire dataset (train + valid). If we don't, we're just hamstringing our model by not using all the available data.\n",
    "\n",
    "Also, we'll create a pipeline for all these steps. For now, we're just exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Valid Split, 80 / 20\n",
    "\n",
    "train, valid = train_test_split(housing, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Plotting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start processing the data, let's visualize the data and see if we can pick out any interesting information that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Scatter Plots for all features vs target\n",
    "\n",
    "for feature in train.columns:\n",
    "    if feature != 'SalePrice':\n",
    "        plt.figure(figsize = (10, 10))\n",
    "        sns.stripplot(x = feature, y = 'SalePrice', hue = None, data = train)\n",
    "        plt.title(f'Sale Price vs {feature}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Let's try to do regression only for numerical features vs target\n",
    "\n",
    "count = 0\n",
    "\n",
    "for feature in train.columns:\n",
    "    if is_numeric_dtype(train[feature]) and feature != 'SalePrice':\n",
    "        count += 1\n",
    "        plt.figure(figsize = (10, 10))\n",
    "        plt.title(f'Sale Price vs {feature}')\n",
    "        sns.regplot(x = feature, y = 'SalePrice', data = train)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial thoughts based on these graphs:\n",
    "1. Many of these features do not seem correlated with our target. However, we must not forget that there may be interactions between these seemingly irrelevant features which ARE correlated with our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start selecting and training models, we have to preprocess our data. This includes things like dealing with missing values, encoding categorical features, and scaling the data to a normal range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "display(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a number of choices for dealing with missing values. Let's begin by dropping features which are missing a lot of data (15% or more).\n",
    "\n",
    "We have to drop from valid, train, and test sets. ONLY USE THE INFO FROM THE TRAIN TO MAKE THE DROPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_null = train.isnull()\n",
    "train_null_sum = train_null[train_null.columns[train_null.sum() > len(train) * .15]].sum()\n",
    "display(train_null_sum)\n",
    "print('')\n",
    "display(train.columns[train.isnull().sum() > len(train) * .15])\n",
    "print('')\n",
    "display(train[train.columns[train.isnull().sum() > len(train) * .15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these features seem peripheral, and not useful to our model, so let's just drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = train.loc[:, train.isnull().sum() > len(train) * .15].columns\n",
    "display(columns_to_drop)\n",
    "train.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "valid.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "test.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "display(train)\n",
    "display(valid)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a good start, but we still have a lot of missing values throughout our data. Let's do simple imputting for the rest, with median for numerical features and mode for categorical features. sklearn has a great function for this built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = train.select_dtypes(exclude=['object']).columns\n",
    "cat_columns = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "display(num_columns)\n",
    "display(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute Categorical Values\n",
    "train.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(train.loc[:, cat_columns])\n",
    "valid.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(valid.loc[:, cat_columns])\n",
    "test.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(test.loc[:, cat_columns])\n",
    "\n",
    "# Impute Numerical Values\n",
    "train.loc[:, num_columns] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(train.loc[:, num_columns])\n",
    "valid.loc[:, num_columns] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(valid.loc[:, num_columns])\n",
    "test.loc[:, num_columns.drop('SalePrice')] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(test.loc[:, num_columns.drop('SalePrice')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the Categorical Features and decide what to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train.select_dtypes(exclude = [np.number])\n",
    "display(train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these variable seem ripe for an ordinal encoding, where we turn each entry into a number, like 0, 1, 2, 3... Note that this is a good idea for categorical features which have some inherent order. But if the feature is not ordered in some sense, then ordinal encoding will confuse the algorithm into believing* there is some inherent order. Certianly there is room for debate on just how ordinal some of these features are.  \n",
    "\n",
    "*No, machines can't think, I'm just anthropomorphising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of features that seem ordinal to me:  \n",
    "1. LotShape: How regular or irregular the property is shaped\n",
    "2. Utilities: Which utilities are available, as a chain of sets, which gives order.\n",
    "3. LandSlope: How sloped the property is\n",
    "4. ExterQual: Evaluates the quality of the material on the exterior \n",
    "5. ExterCond: Evaluates the present condition of the material on the exterior\n",
    "6. BsmtQual: Evaluates the height of the basement\n",
    "7. BsmtCond: Evaluates the general condition of the basement\n",
    "8. BsmtExposure: Refers to walkout or garden level walls\n",
    "9. BsmtFinType1: Rating of basement finished area\n",
    "10. BsmtFinType2: Rating of basement finished area (if multiple types)\n",
    "11. HeatingQC: Heating quality and condition\n",
    "12. CentralAir: Central air conditioning\n",
    "13. Electrical: Electrical system\n",
    "14. KitchenQual: Kitchen quality\n",
    "15. Functional: Home functionality (Assume typical unless deductions are warranted)\n",
    "16. FireplaceQu: Fireplace quality. BUT we already dropped this, as it was missing too many values.\n",
    "17. GarageFinish: Interior finish of the garage\n",
    "18. GarageQual: Garage quality\n",
    "19. GarageCond: Garage condition\n",
    "20. PavedDrive: Paved driveway\n",
    "21. PoolQC: Pool quality. But, we already dropped this, as it was missing too many values.\n",
    "22. Fence Quality. BUT, we already dropped this, as it was missing too many values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sklearn has a nice OrdinalEncoder built in, it can't know the real world values of these descriptions (e.g. for Garage Finish we should have Finished > Rough Finished > Unfinished > No Garage, though even here there could be debate about whether No Garage is truly lower than the other options, or just incomparable). Therefore, I will have to manually encode all of these...  \n",
    "\n",
    "Sigh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are all our maps\n",
    "# I'm following the convention of giving the worst value 0, and the best the maximum\n",
    "\n",
    "lotshape_map = {'Reg':3, 'IR1':2, 'IR2':1, 'IR2':0}\n",
    "utilities_map = {'AllPub':3, 'NoSewr':2, 'NoSeWa':1, 'ELO':0}\n",
    "landslope_map = {'Gtl':2, 'Mod':1, 'Sev':0}\n",
    "exterqual_map = {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0}\n",
    "extercond_map = exterqual_map\n",
    "bsmtqual_map = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n",
    "bsmtcond_map = bsmtqual_map\n",
    "bsmtexposure_map = {'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0}\n",
    "\n",
    "# BSMTFinType1 is a bit strange, because they throw Rec Room in with all the other assesments. I'll set the two averages equal in value, then scale the rest\n",
    "bsmtfintype1_map = {'GLQ':5, 'ALQ':4, 'BLQ':3, 'Rec':4, 'LwQ':2, 'Unf':1, 'NA':0}\n",
    "bsmtfintype2_map = bsmtfintype1_map\n",
    "\n",
    "heatingqc_map = exterqual_map\n",
    "centralair_map = {'Y':1, 'N':0}\n",
    "\n",
    "# Electrical is also weird, as there is a mixed option. I'll just set that to a middle value.\n",
    "electrical_map = {'SBrkr':3, 'FuseA':2, 'FuseF':1, 'FuseP':0}\n",
    "\n",
    "kitchenqual_map = exterqual_map\n",
    "functional_map = {'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0}\n",
    "garagefinish_map = {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0}\n",
    "garagequal_map = bsmtqual_map\n",
    "garagecond_map = bsmtqual_map\n",
    "paveddrive_map = {'Y':2, 'P':1, 'N':0}\n",
    "\n",
    "# Here is a list of the maps I just made, so we can just iterate through them\n",
    "ordinal_maps = [lotshape_map, utilities_map, landslope_map, exterqual_map, extercond_map, bsmtqual_map, bsmtcond_map, bsmtexposure_map, bsmtfintype1_map, bsmtfintype2_map, heatingqc_map, centralair_map, electrical_map, kitchenqual_map, functional_map, garagefinish_map, garagequal_map, garagecond_map, paveddrive_map]\n",
    "\n",
    "# Here's our list of columns to map, so we can just iterate through them\n",
    "\n",
    "ordinal_features = ['LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ordinal_maps)):\n",
    "    train[ordinal_features[i]] = train[ordinal_features[i]].map(ordinal_maps[i])\n",
    "    valid[ordinal_features[i]] = valid[ordinal_features[i]].map(ordinal_maps[i])\n",
    "    test[ordinal_features[i]] = test[ordinal_features[i]].map(ordinal_maps[i])\n",
    "\n",
    "display(train[ordinal_features])\n",
    "display(valid[ordinal_features])\n",
    "display(test[ordinal_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's see what categorical features are left to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train.select_dtypes(exclude = [np.number])\n",
    "display(train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, I'm not sure what to do with these features for right now, so let's DROP EM ALL!!!! HAHAHAHAHAHAHAHAHA.\n",
    "\n",
    "One-Hot would be my next choice, but who has time for that???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "display(cat_columns)\n",
    "\n",
    "train.drop(cat_columns, inplace = True, axis = 1)\n",
    "valid.drop(cat_columns, inplace = True, axis = 1)\n",
    "test.drop(cat_columns, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the correlation coefficients for each pair of Numerical Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()\n",
    "display(corr)\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(corr)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.clustermap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. There's a lot more to consider here, but for now let's just focus on numerical features that are highly correlated with our target value.  \n",
    "\n",
    "Yes, this is a gross simplification. But we'll do multivariate analysis later. I promise!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(corr['SalePrice'].sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our model to just correlation coefficients .5 or above. That will bring us down to 14 numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sp = corr['SalePrice']\n",
    "\n",
    "print('')\n",
    "columns_to_drop = corr_sp.loc[corr_sp < .5].index\n",
    "display(columns_to_drop)\n",
    "\n",
    "train.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "valid.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "test.drop(columns_to_drop, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to do some feature scaling!! Normalize this stuff.\n",
    "\n",
    "For this, and later steps, we'll need to split out our X and y values.\n",
    "\n",
    "We'll need to reverse transform our predictions before the end, so let's not forget that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, :-1]\n",
    "y_train = train.iloc[:, -1]\n",
    "\n",
    "X_valid = valid.iloc[:, :-1]\n",
    "y_valid = valid.iloc[:, -1]\n",
    "\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can scale the data. As always, only fit the the scaler on the TRAIN data, to prevent leakage.\n",
    "\n",
    "Also, we have to do some reshaping to scale the y values.\n",
    "\n",
    "Ok, having some trouble with scaling the data. Can't scale the y values for some reason.\n",
    "\n",
    "I'll work on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ML models learn much better from data that is similarly scaled. So I will use a normal scaler to, well, normalize the data ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Also, scale the data!!\n",
    "# Here, we're only scaling the domain, not the range. Should investigate whether it's worth it to scale the range as well\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = pd.DataFrame(scaler.transform(X_train),columns = X_train.columns)\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid),columns = X_valid.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test),columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaler = StandardScaler()\n",
    "# y_scaler = StandardScaler()\n",
    "# X_scaler.fit(X_train)\n",
    "# y_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "# X_train = X_scaler.transform(X_train)\n",
    "# X_valid = X_scaler.transform(X_valid)\n",
    "# X_test = X_scaler.transform(X_test)\n",
    "\n",
    "# y_train = np.squeeze(y_scaler.transform(y_train.reshape(-1, 1)))\n",
    "# y_valid = np.squeeze(y_scaler.transform(y_valid.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we're down to a manageable number of features, let's start training and comparing models. We'll start with decision tree, random forest, XGBoost, and LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "display(f'RMSE for Decision Tree Model is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_decisiontree.csv',index=False, header =1)\n",
    "\n",
    "plot_tree(model)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I guess this isn't the worst, but given that the average sale price is 181286.518836, we're looking at about 20% error on average.\n",
    "\n",
    "Ok, let's upload to Kaggle and compare:\n",
    "\n",
    "Decision Tree Model got a score of 0.23263, which is substantially worse than our current best of 0.13284  \n",
    "\n",
    "Our best came from XGBoost and the same procedure as the Kaggle tutorials, plus lots of hyperparameter tuning.\n",
    "\n",
    "Look at that decision tree. Clearly we're substantially overfitting the training data. The methods below will help with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone knows random forest is way better than simple decision tree!!! Let's put that to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "display(f'RMSE for Random Forest Model is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_randomforest.csv',index=False, header =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so far we're definitely beating Decision Tree. Let's also sumbit to Kaggle to see how we're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2 got a score of 0.16133, which is a SUBSTANTIAL improvement over the decision tree. However, it's still a bit worse than our current best of 0.13284  \n",
    "\n",
    "Our best came from XGBoost and the same procedure as the Kaggle tutorials, plus lots of hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost was the hottest model a few years ago. However, it's said to have been surpased by LightGBM these days. Let's find out!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=20, verbose = False)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "\n",
    "print('')\n",
    "\n",
    "display(f'RMSE for XGBoost Model is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_xgb.csv',index=False, header =1)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "xgb.plot_importance(model, height = .75, grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the new hotness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=20, verbose = False)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "\n",
    "print('')\n",
    "\n",
    "display(f'RMSE for LightGBM Model is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_lgb.csv',index=False, header =1)\n",
    "\n",
    "lgb.plot_importance(model, height=.75, figsize = (10, 10), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, even better than decision tree, random forest, or even XGBoost!! Let's also sumbit to Kaggle to see how we're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model4 got a score of 0.16308, which is slightly worse than decision tree. It's also still a bit worse than our current best of 0.13284  \n",
    "\n",
    "Our best came from XGBoost and the same procedure as the Kaggle tutorials, plus lots of hyperparameter tuning.\n",
    "\n",
    "We could likely improve with some hyperparameter tuning, but there are some other big picture improvements we could work on, like:\n",
    "\n",
    "1.  k-fold validation\n",
    "2.  Better feature generation and selection\n",
    "3.  Then maybe some hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Big Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done about all we can with my first preprocessing attempt. Let's see how LightGBM does with minimal preprocessing. Just let it decide what to do. I'm going to call this model the \"Big Dump\" model, because we're basically just dumping everything into LightGBM and letting it figure out the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(f'{filepath}train.csv')\n",
    "test = pd.read_csv(f'{filepath}test.csv')\n",
    "\n",
    "train, valid = train_test_split(housing, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, ordinal encoder doesn't work with NaN values. Crap. Let's do the missing value thing from above first\n",
    "\n",
    "# Drop heavily empty columns\n",
    "columns_to_drop = train.columns[train.isnull().sum() > len(train) * .15]\n",
    "train.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "valid.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "test.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "# Also, drop the IDs\n",
    "train.drop('Id', inplace = True, axis = 1)\n",
    "valid.drop('Id', inplace = True, axis = 1)\n",
    "test.drop('Id', inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "# Impute the rest\n",
    "\n",
    "num_columns = train.select_dtypes(exclude=['object']).columns\n",
    "cat_columns = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute Categorical Values\n",
    "train.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(train.loc[:, cat_columns])\n",
    "valid.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(valid.loc[:, cat_columns])\n",
    "test.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(test.loc[:, cat_columns])\n",
    "\n",
    "# Impute Numerical Values\n",
    "train.loc[:, num_columns] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(train.loc[:, num_columns])\n",
    "valid.loc[:, num_columns] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(valid.loc[:, num_columns])\n",
    "test.loc[:, num_columns.drop('SalePrice')] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(test.loc[:, num_columns.drop('SalePrice')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM says it can handle categorical features without the need for integer or one-hot, but it wasn't doing it for me. Piece of trash.\n",
    "# LightGBM is supposed to do well with integer encoding, so let's try that\n",
    "# Was having trouble, so I encoded each seperately, when I really should only encode based on the train data\n",
    "\n",
    "code = OrdinalEncoder()\n",
    "\n",
    "train.loc[:, cat_columns] = code.fit_transform(train.loc[:, cat_columns])\n",
    "valid.loc[:, cat_columns] = code.fit_transform(valid.loc[:, cat_columns])\n",
    "test.loc[:, cat_columns] = code.fit_transform(test.loc[:, cat_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, :-1]\n",
    "y_train = train.iloc[:, -1]\n",
    "\n",
    "X_valid = valid.iloc[:, :-1]\n",
    "y_valid = valid.iloc[:, -1]\n",
    "\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, scale the data!!\n",
    "# Here, we're only scaling the domain, not the range. Should investigate whether it's worth it to scale the range as well\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = pd.DataFrame(scaler.transform(X_train),columns = X_train.columns)\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid),columns = X_valid.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test),columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train)\n",
    "display(X_valid)\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=20, verbose = False)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "\n",
    "print('')\n",
    "\n",
    "display(f'RMSE for Big Dump LightGBM is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_lgbdump.csv',index=False, header =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I've run this a few times, and it keeps changing. Let's kaggle it and see how we do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle: .14081\n",
    "    \n",
    "Well crap, this is better than even my attempt with all the encoding and stuff. What a waste of time! Still, that's probably because I dropped those other categorical columns.\n",
    "\n",
    "Let's try tuning this bad boy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search the Big Dump Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {'num_leaves': range(50, 60), \n",
    "             'learning_rate': [.1],\n",
    "#              'min_child_samples': range(100, 500), \n",
    "#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "#              'subsample': (.2, .3, .4, .5, .6, .7, .8), \n",
    "#              'colsample_bytree': (.2, .3, .4, .5, .6),\n",
    "#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(max_depth=-1, silent=True, n_jobs=3, n_estimators=5000, verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "gs = GridSearchCV(\n",
    "            estimator=model, \n",
    "            param_grid=param_test, \n",
    "            scoring='neg_mean_squared_error',\n",
    "            #cv=5,\n",
    "            refit=True,\n",
    "            verbose=True,\n",
    "            n_jobs = 3\n",
    "            )\n",
    "\n",
    "gs.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = 'rmse', verbose = False, early_stopping_rounds = 30)\n",
    "print(f'Best score reached: {gs.best_score_} with params: {gs.best_params_} ')\n",
    "\n",
    "model = lgb.LGBMRegressor(**gs.best_params_)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=30, verbose = False)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "\n",
    "print('')\n",
    "\n",
    "display(f'RMSE for LightGBM Big Dump Grid Search with parameters {gs.best_params_} is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_lgbdumpgs.csv',index=False, header =1)\n",
    "\n",
    "lgb.plot_importance(model, height=.75, figsize = (10, 10), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of things that performed decently:  \n",
    "\n",
    "1. learning_rate = .1, num_leaves = 9, early_stopping_rounds = 30, RMSE for model8 is 19088.782686261766'\n",
    "2. {'early_stopping_rounds': 100, 'learning_rate': 0.01, 'num_leaves': 11}, 'RMSE for model8 is 19088.782686261766'\n",
    "3. {'learning_rate': 0.01, 'num_leaves': 6, early_stopping rounds = 50}, 'RMSE for model8 is 36324.154902046415'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kaggle: .14039\n",
    "A little better, still not beating our og. Let's go nuts with hyper tuning.\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this did eventually give us a decent result, but it's SLOW AS HECK to grid search, and I'm getting bored. Let's RANDOM SEARCH BABY!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search the Big Dump Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {'num_leaves': range(2, 20), \n",
    "             'learning_rate': [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n",
    "              'early_stopping_rounds' : range(10, 200, 10)\n",
    "#              'min_child_samples': range(100, 500), \n",
    "#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "#              'subsample': (.2, .3, .4, .5, .6, .7, .8), \n",
    "#              'colsample_bytree': (.2, .3, .4, .5, .6),\n",
    "#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(max_depth=-1, silent=True, n_jobs=3, n_estimators=5000, verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "            estimator=model, \n",
    "            param_distributions=param_test, \n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            refit=True,\n",
    "            verbose=True,\n",
    "            n_iter = 1000,\n",
    "            n_jobs = 3\n",
    "            )\n",
    "\n",
    "rs.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = 'rmse', verbose = False)\n",
    "\n",
    "print(f'Best score reached: {rs.best_score_} with params: {rs.best_params_} ')\n",
    "\n",
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(**rs.best_params_)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose = False)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "\n",
    "print('')\n",
    "\n",
    "display(f'RMSE for LightGBM Big Dump Random Search with parameters {rs.best_params_} is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_lgbdumprs.csv',index=False, header =1)\n",
    "\n",
    "lgb.plot_importance(model, height=.75, figsize = (10, 10), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of things that performed decently:  \n",
    "\n",
    "1. \"RMSE for model9 with parameters {'num_leaves': 10, 'learning_rate': 0.02, 'early_stopping_rounds': 180} is 36890.537657012734\"\n",
    "2. \"RMSE for model9 with parameters {'num_leaves': 11, 'learning_rate': 0.01, 'early_stopping_rounds': 30} is 47800.831296522185\"\n",
    "3. \"RMSE for model9 with parameters {'num_leaves': 8, 'learning_rate': 0.02, 'early_stopping_rounds': 30} is 38031.55374833302\"\n",
    "4. \"RMSE for model9 with parameters {'num_leaves': 8, 'learning_rate': 0.2, 'early_stopping_rounds': 10} is 28506.639199674595\"\n",
    "5. \"RMSE for model9 with parameters {'num_leaves': 14, 'learning_rate': 0.1, 'early_stopping_rounds': 10} is 28387.987302916474\"\n",
    "6. \"RMSE for model9 with parameters {'num_leaves': 15, 'learning_rate': 0.1, 'early_stopping_rounds': 10} is 28320.97315991774\n",
    "7. \"RMSE for model9 with parameters {'num_leaves': 3, 'learning_rate': 0.06, 'early_stopping_rounds': 40} is 36486.701188774045\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of things that performed decently:  \n",
    "\n",
    "1. learning_rate = .1, num_leaves = 9, early_stopping_rounds = 30, RMSE for model8 is 19088.782686261766'\n",
    "2. {'early_stopping_rounds': 100, 'learning_rate': 0.01, 'num_leaves': 11}, 'RMSE for model8 is 19088.782686261766'\n",
    "3. {'learning_rate': 0.01, 'num_leaves': 6, early_stopping rounds = 50}, 'RMSE for model8 is 36324.154902046415'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kaggle: .14039\n",
    "A little better, still not beating our og. Let's go nuts with hyper tuning.\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kaggle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprocessing the Data 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some other ways of preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my first attempt, I'll keep everything but very empty columns, use my encoding where possible, otherwise use integer encoding.\n",
    "\n",
    "Also, no more validation split. Use CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing = pd.read_csv(f'{filepath}train.csv')\n",
    "test = pd.read_csv(f'{filepath}test.csv')\n",
    "\n",
    "columns_to_drop = housing.columns[housing.isnull().sum() > len(housing) * .15]\n",
    "housing.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "test.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "num_columns = housing.select_dtypes(exclude=['object']).columns\n",
    "cat_columns = housing.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute Categorical Values\n",
    "housing.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(housing.loc[:, cat_columns])\n",
    "test.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(test.loc[:, cat_columns])\n",
    "\n",
    "# Impute Numerical Values\n",
    "housing.loc[:, num_columns] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(housing.loc[:, num_columns])\n",
    "test.loc[:, num_columns.drop('SalePrice')] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(test.loc[:, num_columns.drop('SalePrice')])\n",
    "\n",
    "lotshape_map = {'Reg':3, 'IR1':2, 'IR2':1, 'IR2':0}\n",
    "utilities_map = {'AllPub':3, 'NoSewr':2, 'NoSeWa':1, 'ELO':0}\n",
    "landslope_map = {'Gtl':2, 'Mod':1, 'Sev':0}\n",
    "exterqual_map = {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0}\n",
    "extercond_map = exterqual_map\n",
    "bsmtqual_map = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n",
    "bsmtcond_map = bsmtqual_map\n",
    "bsmtexposure_map = {'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0}\n",
    "bsmtfintype1_map = {'GLQ':5, 'ALQ':4, 'BLQ':3, 'Rec':4, 'LwQ':2, 'Unf':1, 'NA':0}\n",
    "bsmtfintype2_map = bsmtfintype1_map\n",
    "heatingqc_map = exterqual_map\n",
    "centralair_map = {'Y':1, 'N':0}\n",
    "electrical_map = {'SBrkr':3, 'FuseA':2, 'FuseF':1, 'FuseP':0}\n",
    "kitchenqual_map = exterqual_map\n",
    "functional_map = {'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0}\n",
    "garagefinish_map = {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0}\n",
    "garagequal_map = bsmtqual_map\n",
    "garagecond_map = bsmtqual_map\n",
    "paveddrive_map = {'Y':2, 'P':1, 'N':0}\n",
    "\n",
    "ordinal_maps = [lotshape_map, utilities_map, landslope_map, exterqual_map, extercond_map, bsmtqual_map, bsmtcond_map, bsmtexposure_map, bsmtfintype1_map, bsmtfintype2_map, heatingqc_map, centralair_map, electrical_map, kitchenqual_map, functional_map, garagefinish_map, garagequal_map, garagecond_map, paveddrive_map]\n",
    "ordinal_features = ['LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']\n",
    "\n",
    "for i in range(len(ordinal_maps)):\n",
    "    housing[ordinal_features[i]] = housing[ordinal_features[i]].map(ordinal_maps[i])\n",
    "    test[ordinal_features[i]] = test[ordinal_features[i]].map(ordinal_maps[i])\n",
    "\n",
    "cat_columns = housing.select_dtypes(include=['object']).columns\n",
    "\n",
    "code = OrdinalEncoder()\n",
    "\n",
    "housing.loc[:, cat_columns] = code.fit_transform(housing.loc[:, cat_columns])\n",
    "test.loc[:, cat_columns] = code.fit_transform(test.loc[:, cat_columns])\n",
    "\n",
    "X_train = housing.iloc[:, :-1]\n",
    "y_train = housing.iloc[:, -1]\n",
    "\n",
    "X_test = test\n",
    "\n",
    "# Also, scale the data!!\n",
    "# Here, we're only scaling the domain, not the range. Should investigate whether it's worth it to scale the range as well\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = pd.DataFrame(scaler.transform(X_train),columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test),columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "\n",
    "param_test = {\n",
    "                'num_leaves': range(2, 20), \n",
    "                'learning_rate': [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1],\n",
    "                'n_estimators' : range(100, 5000, 10)\n",
    "            }\n",
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(silent=True, n_jobs=3, verbose = -1)\n",
    "\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "            estimator=model, \n",
    "            param_distributions=param_test, \n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=5,\n",
    "            refit=True,\n",
    "            verbose=True,\n",
    "            n_iter = n_iter,\n",
    "            n_jobs = 3\n",
    "            )\n",
    "\n",
    "rs.fit(X_train, y_train, verbose = False)\n",
    "\n",
    "print(f'Best score reached: {rs.best_score_} with params: {rs.best_params_} ')\n",
    "\n",
    "model = lgb.LGBMRegressor(**rs.best_params_)\n",
    "\n",
    "model.fit(X_train, y_train, verbose = False)\n",
    "\n",
    "predict = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_reprelgb1.csv',index=False, header =1)\n",
    "\n",
    "lgb.plot_importance(model, height = .75, figsize = (10, 10), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of things that performed decently:  \n",
    "\n",
    "1. \"RMSE for model7 with parameters {'num_leaves': 7, 'learning_rate': 0.1, 'early_stopping_rounds': 30} is 26200.168821416515\"\n",
    "2. \"RMSE for model7 with parameters {'num_leaves': 12, 'learning_rate': 0.3, 'early_stopping_rounds': 40} is 35874.30209147329\"\n",
    "3. \"RMSE for model7 with parameters {'num_leaves': 9, 'learning_rate': 0.02, 'early_stopping_rounds': 40} is 27146.89532722071\"\n",
    "4. \"RMSE for model7 with parameters {'num_leaves': 18, 'learning_rate': 0.08, 'early_stopping_rounds': 40} is 24956.638132730524\"\n",
    "\n",
    "\n",
    "5. [Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.5min finished  \n",
    "Best score reached: -28173.823200826864 with params: {'num_leaves': 14, 'learning_rate': 0.06, 'early_stopping_rounds': 40}   \n",
    "\"RMSE for Repreprocessed LightGBM Random Search with parameters {'num_leaves': 14, 'learning_rate': 0.06, 'early_stopping_rounds': 40} is $27616.147114770138\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Kaggle: 15817\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprocessing the Data 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reprocess one more time, with a few changes:\n",
    "1. No validation set. We'll use the cross validation built into random search to validate our model.\n",
    "2. One hot encoding for columns that I haven't hand encoded.\n",
    "3. Train the final model on the whole dataset. Again, no validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(f'{filepath}train.csv')\n",
    "test = pd.read_csv(f'{filepath}test.csv')\n",
    "\n",
    "train = housing\n",
    "\n",
    "columns_to_drop = train.columns[train.isnull().sum() > len(train) * .15]\n",
    "train.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "test.drop(columns_to_drop, inplace = True, axis = 1)\n",
    "\n",
    "num_columns = train.select_dtypes(exclude=['object']).columns\n",
    "cat_columns = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute Categorical Values\n",
    "train.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(train.loc[:, cat_columns])\n",
    "test.loc[:, cat_columns] = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent',verbose=0).fit_transform(test.loc[:, cat_columns])\n",
    "\n",
    "# Impute Numerical Values\n",
    "train.loc[:, num_columns] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(train.loc[:, num_columns])\n",
    "test.loc[:, num_columns.drop('SalePrice')] = SimpleImputer(missing_values = np.nan, strategy = 'mean',verbose=0).fit_transform(test.loc[:, num_columns.drop('SalePrice')])\n",
    "\n",
    "lotshape_map = {'Reg':3, 'IR1':2, 'IR2':1, 'IR2':0}\n",
    "utilities_map = {'AllPub':3, 'NoSewr':2, 'NoSeWa':1, 'ELO':0}\n",
    "landslope_map = {'Gtl':2, 'Mod':1, 'Sev':0}\n",
    "exterqual_map = {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0}\n",
    "extercond_map = exterqual_map\n",
    "bsmtqual_map = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n",
    "bsmtcond_map = bsmtqual_map\n",
    "bsmtexposure_map = {'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0}\n",
    "bsmtfintype1_map = {'GLQ':5, 'ALQ':4, 'BLQ':3, 'Rec':4, 'LwQ':2, 'Unf':1, 'NA':0}\n",
    "bsmtfintype2_map = bsmtfintype1_map\n",
    "heatingqc_map = exterqual_map\n",
    "centralair_map = {'Y':1, 'N':0}\n",
    "electrical_map = {'SBrkr':3, 'FuseA':2, 'FuseF':1, 'FuseP':0}\n",
    "kitchenqual_map = exterqual_map\n",
    "functional_map = {'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0}\n",
    "garagefinish_map = {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0}\n",
    "garagequal_map = bsmtqual_map\n",
    "garagecond_map = bsmtqual_map\n",
    "paveddrive_map = {'Y':2, 'P':1, 'N':0}\n",
    "\n",
    "ordinal_maps = [lotshape_map, utilities_map, landslope_map, exterqual_map, extercond_map, bsmtqual_map, bsmtcond_map, bsmtexposure_map, bsmtfintype1_map, bsmtfintype2_map, heatingqc_map, centralair_map, electrical_map, kitchenqual_map, functional_map, garagefinish_map, garagequal_map, garagecond_map, paveddrive_map]\n",
    "ordinal_features = ['LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']\n",
    "\n",
    "for i in range(len(ordinal_maps)):\n",
    "    train[ordinal_features[i]] = train[ordinal_features[i]].map(ordinal_maps[i])\n",
    "    test[ordinal_features[i]] = test[ordinal_features[i]].map(ordinal_maps[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where things get interesting. I think there may be a different number of unique entries for the remaining categorical columns. Which means that when I one-hot, the number of features won't match. Let's check and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = train.select_dtypes(include=['object']).columns\n",
    "\n",
    "for feature in cat_columns:    \n",
    "    difference = set(train[feature].unique()) - set(test[feature].unique())\n",
    "    if difference != set():\n",
    "        display(feature)\n",
    "        display(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! We've found the culprits! Let's count up how many of each type appear in both train and test, to help us decide what to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in cat_columns:    \n",
    "    difference = set(train[feature].unique()) - set(test[feature].unique())\n",
    "    if difference != set():\n",
    "        for value in difference:\n",
    "            diff = len(train[train[feature] == value].index) - len(test[test[feature] == value].index)\n",
    "            print(f'There are {diff} more of {value} in train than test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I suspected, all of the additional values occur in the train set, which makes sense, as that is a much larger set than the test set. So, after we one hot, let's manually add in the missing values above as columns of zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=cat_columns)\n",
    "test = pd.get_dummies(test, columns=cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in cat_columns:    \n",
    "    difference = set(train[feature].unique()) - set(test[feature].unique())\n",
    "    if difference != set():\n",
    "        for value in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, :-1]\n",
    "y_train = train.iloc[:, -1]\n",
    "\n",
    "X_test = test\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "model.fit(X_train, y_train, verbose = False)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_reprelgb2.csv',index=False, header =1)\n",
    "\n",
    "lgb.plot_importance(model, height=.75, figsize = (10, 10), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search Preprocessing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {'num_leaves': range(2, 20), \n",
    "             'learning_rate': [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1],\n",
    "              'early_stopping_rounds' : range(10, 50, 10)\n",
    "#              'min_child_samples': range(100, 500), \n",
    "#              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "#              'subsample': (.2, .3, .4, .5, .6, .7, .8), \n",
    "#              'colsample_bytree': (.2, .3, .4, .5, .6),\n",
    "#              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "#              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "model = lgb.LGBMRegressor(max_depth=-1, silent=True, n_jobs=3, n_estimators=5000, verbose = -1)\n",
    "\n",
    "\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "            estimator=model, \n",
    "            param_distributions=param_test, \n",
    "            scoring='neg_mean_squared_log_error',\n",
    "            cv=5,\n",
    "            refit=True,\n",
    "            verbose=True,\n",
    "            n_iter = 10,\n",
    "            n_jobs = 3\n",
    "            )\n",
    "\n",
    "rs.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = 'rmse', verbose = False)\n",
    "\n",
    "print(f'Best score reached: {rs.best_score_} with params: {rs.best_params_} ')\n",
    "\n",
    "model = lgb.LGBMRegressor(**rs.best_params_)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose = False)\n",
    "\n",
    "valid_pred = model.predict(X_valid)\n",
    "error = np.sqrt(mse(valid_pred, y_valid))\n",
    "\n",
    "print('')\n",
    "\n",
    "display(f'RMSE for Repreprocessed LightGBM Random Search with parameters {rs.best_params_} is ${error}')\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_id, 'SalePrice': test_pred})\n",
    "\n",
    "output.to_csv(f'{sub_filepath}advhousesub_reprelgbrs2.csv',index=False, header =1)\n",
    "\n",
    "lgb.plot_importance(model, height=.75, figsize = (10, 10), grid = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kaggle: \n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we've had a lot of fun, but what have we learned?  \n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm ready to move on to other research and other datasets, but what would I do if I were to continue with this dataset?  \n",
    "\n",
    "1. Develop pipelines for all of my procedures. sklearn has some cool built in functions for creating pipelines.\n",
    "2. Use dimensionality reduction to try to look for the most important features / linear combinations of features.\n",
    "3. Use L1 / Lasso to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
